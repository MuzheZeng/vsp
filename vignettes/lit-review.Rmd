---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

https://research.ics.aalto.fi/ica/fastica/fp.shtml -- fast ICA:

> It uses a fixed-point iteration scheme that has been found in independent experiments to be 10-100 times faster than conventional gradient descent methods for ICA. 


### fast ICA

- typically preprocess via whitening (this can be accomplish via PCA)

diff between PCA and ICA rundown:
http://compneurosci.com/wiki/images/4/42/Intro_to_PCA_and_ICA.pdf


### relationship of ICA and varimax


### GPA algorithm for varimax

# scikit learn implemenation 

# https://github.com/scikit-learn/scikit-learn/blob/b7f0fd5d046d2221fbe1f2766d79bae0388c1fae/sklearn/decomposition/factor_analysis.py#L373

```{r}
x <- matrix(1:12, nrow = 3)
k <- max(dim(x))

# find the closest orthonormal matrix to x
project_orthonormal <- function(x) {
  s <- svd(x)
  s$u %*% t(s$v)
}

vrmx_gradient <- function(x) {
  
}

TT <- diag(n)


# subsample x to move from projected gradient descent to project sgd

x

varimax2 <- function (x, eps = 1e-05) {
  nc <- ncol(x)
  if (nc < 2)
    return(x)
  p <- nrow(x)
  TT <- diag(nc)
  d <- 0
  
  browser()
  
  for (i in 1L:1000L) {
    z <- x %*% TT
    B <- t(x) %*% (z^3 - z %*% diag(drop(rep(1, p) %*% z^2))/p)

    # projection to recover orthonormality. randomized svd!
    sB <- La.svd(B)
    TT <- sB$u %*% sB$vt

    # convergence criteria
    dpast <- d
    d <- sum(sB$d)
    if (d < dpast * (1 + eps))
      break
  }
  TT
}

varimax2(x)

```

# GP slides: http://www.stat.ucla.edu/research/gpa/jsm2003.pdf


ten Berge 1984 says that the LP method solution to the simultaneous diagonalization of symmetric matrices problem is equivalent to Kaiser's method, which is also equivalent to the gradient projection method